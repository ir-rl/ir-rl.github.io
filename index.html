
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Internally Rewarded Reinforcement Learning</title>

    <meta name="description" content="Internally Rewarded Reinforcement Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <link rel="stylesheet" href="css/bulma-slider.min.css">
    <link rel="stylesheet" href="css/index.css"> 

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/bulma-carousel.min.js"></script>
    <script src="js/bulma-slider.min.js"></script>
    <script src="js/index.js"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <br>
                <b>Internally Rewarded Reinforcement Learning</b>
            </h2>
            <br>
            <h4 class="text-center">
                <!-- <a href="https://icml.cc/Conferences/2023">  -->
                International Conference on Machine Learning (ICML), 2023
                <!-- </a> -->
            </h4>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/li.html">Mengdi Li*</a></li> <li><a href="https://xf-zhao.github.io/">Xufeng Zhao*</a></li> <li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/lee.html">Jae Hee Lee</a></li> <li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html">Cornelius Weber</a></li> <li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a></li>
                <br>
                * Equal contribution
                <br><br>
                    <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm.html">
                    <image src="img/new-wtm-logo-white-150x150.jpg" height="40px"> Knowledge Technology Group</a>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-2 col-md-offset-5 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2302.00270.pdf">
                            <image src="img/paper_small.png" height="60px">
                            <image src="img/new.png" height="20px" class="imtip">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <!-- Youtube video -->
                        <!-- <li>
                            <a href="https://youtu.be/xxx">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li> -->
                        
                         <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <image src="img/new.png" height="20px" class="imtip">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> 
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- Youtube video -->
                <!-- <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/rMMeMTWmT0k" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div> -->
                <h3 class="text-center">
                    <b>Abstract</b>
                </h3>
                <p class="text-justify">
                     We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an untrained policy impedes discriminator learning. We call this learning setting <em>Internally Rewarded Reinforcement Learning</em> (IRRL) as the reward is not provided directly by the environment but internally by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance compared with baselines in diverse tasks. 
                </p>
            </div>
        </div>


        <!-- Main figures -->
        <br>
        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <!-- <a href="https://arxiv.org/pdf/2302.00270.pdf"> -->
                            <image src="img/agent-environment-interaction-loop.png" height="314px">
                                <!-- <h4><strong>Paper</strong></h4> -->
                            <!-- </a> -->
                        </li>

                         <li>
                            <!-- <a href=""> -->
                            <image src="img/noisy-rewards.png" height="170px">
                                <!-- <image src="img/new.png" height="20px" class="imtip"> -->
                                <!-- <h4><strong>Code</strong></h4> -->
                            <!-- </a> -->
                        </li> 
                    </ul>
                </div>
        </div>


        <!-- Samples: RAM -->
        <div class="row"> 
            <div class="col-md-12 text-center">
                <h3>
                    <b>Experiments</b>
                </h3>
                <p class="text-justify">
                    We compare the proposed clipped linear reward with conventional clipped logarithmic reward and accuracy-based reward on various tasks. 
                    Some experimental results on the hard attention task of digit recognition and robotic object counting are presented below.
                    Please check our paper for details about the models and experimental setups. 
                </p>
                <h4 class="text-justify">
                    <b>RAM on the digit recognition task</b>
                </h4>
                <p class="text-justify">
                    Visulize the performance of RAM models trained by different reward functions. Use the slider to change the training epoch when the model is evaluated. Samples are randomly selected from the evalution dataset.
                    The starting, intermediate, and stopping glimpses are represented by yellow, green, and red boxes respectively. 
                </p>
                <ul class="list-inline">
                    <li>
                        <p>Select a random seed: </p>
                    </li>
                    <li>
                        <div class="select is-normal text-justify">
                            <select id="single-menu-seeds-ram" onchange="preloadAndSetImages_ram()">
                                <option value="1" selected="selected">1</option>
                                <option value="2">2</option>
                                <option value="3">3</option>
                            </select>
                        </div>
                    </li>
                </ul>
            </div>

            <div class="col-md-12 text-center">
                <div class="col-md-6 text-center">
                    <div class="sliding-image-wrapper" id="sliding-image-wrapper-ram-clipped-linear">
                        Loading...
                    </div>
                    <input class="slider has-output-tooltip is-fullwidth is-large is-info"
                        style="display: block; margin-left: auto; margin-right: auto; width: 85%;"
                        id="slider-ram-clipped-linear"
                        step="50" min="0" max="1450" value="0" type="range">
                    <output for="slider-ram-clipped-linear">0</output>
                    <p class="text-center">Clipped linear reward</p>
                </div>
                <div class="col-md-6 text-center">
                    <div class="sliding-image-wrapper" id="sliding-image-wrapper-ram-clipped-log">
                        Loading...
                    </div>
                    <input class="slider has-output-tooltip is-fullwidth is-large is-info"
                        style="display: block; margin-left: auto; margin-right: auto; width: 85%;"
                        id="slider-ram-clipped-log"
                        step="50" min="0" max="1450" value="0" type="range">
                    <output for="slider-ram-clipped-log">0</output>
                    <p class="text-center">Clipped logarithmic reward</p>
                </div>
            </div>
            <div class="col-md-12 text-center">
                <div class="col-md-6 text-center">
                    <div class="sliding-image-wrapper" id="sliding-image-wrapper-ram-acc">
                        Loading...
                    </div>
                    <input class="slider has-output-tooltip is-fullwidth is-large is-info"
                        style="display: block; margin-left: auto; margin-right: auto; width: 85%;"
                        id="slider-ram-acc"
                        step="50" min="0" max="1450" value="0" type="range">
                    <output for="slider-ram-acc">0</output>
                    <p class="text-center">Accuracy-based reward</p>
                </div>
                <div class="col-md-6">
                    <center> 
                        <image src="img/plots/digit-recognition-baseline-comparison-ram.jpg" style="max-width: 75%;max-height: 75%; ">
                        <p class="text-center">Training curves</p>
                    </center>
                </div>
            </div>
        </div>
            
        <!-- Samples: DT-RAM -->
        <div class="row"> 
            <div class="col-md-12 text-center">
                <h4 class="text-justify">
                    <b>DT-RAM on the digit recognition task</b>
                </h4>
                <p class="text-justify">
                    Visulize the performance of DT-RAM models trained by different reward functions. Use the slider to change the training epoch when the model is evaluated. Samples are randomly selected from the evalution dataset.
                    The starting, intermediate, and stopping glimpses are represented by yellow, green, and red boxes respectively. 
                </p>
                <ul class="list-inline">
                    <li>
                        <p>Select a random seed: </p>
                    </li>
                    <li>
                        <div class="select is-normal text-justify">
                            <select id="single-menu-seeds-dt-ram" onchange="preloadAndSetImages_dt_ram()">
                                <option value="1" selected="selected">1</option>
                                <option value="2">2</option>
                                <option value="3">3</option>
                            </select>
                        </div>
                    </li>
                </ul>
            </div>

            <div class="col-md-12 text-center">
                <div class="col-md-6 text-center">
                    <div class="sliding-image-wrapper" id="sliding-image-wrapper-dt-ram-clipped-linear">
                        Loading...
                    </div>
                    <input class="slider has-output-tooltip is-fullwidth is-large is-info"
                        style="display: block; margin-left: auto; margin-right: auto; width: 85%;"
                        id="slider-dt-ram-clipped-linear"
                        step="50" min="0" max="1450" value="0" type="range">
                    <output for="slider-dt-ram-clipped-linear">0</output>
                    <p class="text-center">Clipped linear reward</p>
                </div>
                <div class="col-md-6 text-center">
                    <div class="sliding-image-wrapper" id="sliding-image-wrapper-dt-ram-clipped-log">
                        Loading...
                    </div>
                    <input class="slider has-output-tooltip is-fullwidth is-large is-info"
                        style="display: block; margin-left: auto; margin-right: auto; width: 85%;"
                        id="slider-dt-ram-clipped-log"
                        step="50" min="0" max="1450" value="0" type="range">
                    <output for="slider-dt-ram-clipped-log">0</output>
                    <p class="text-center">Clipped logarithmic reward</p>
                </div>
            </div>
            <div class="col-md-12 text-center">
                <div class="col-md-6 text-center">
                    <div class="sliding-image-wrapper" id="sliding-image-wrapper-dt-ram-acc">
                        Loading...
                    </div>
                    <input class="slider has-output-tooltip is-fullwidth is-large is-info"
                        style="display: block; margin-left: auto; margin-right: auto; width: 85%;"
                        id="slider-dt-ram-acc"
                        step="50" min="0" max="1450" value="0" type="range">
                    <output for="slider-dt-ram-acc">0</output>
                    <p class="text-center">Accuracy-based reward</p>
                </div>
                <div class="col-md-6">
                    <center> 
                        <image src="img/plots/digit-recognition-baseline-comparison-accuracy-dt-ram.jpg" style="max-width: 75%;max-height: 75%; ">
                        <p class="text-center">Training curves</p>
                    </center>
                </div>
            </div>
        </div>

         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3> 
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>

@article{li2023irrl,
title={Internally Rewarded Reinforcement Learning}, 
author={Li, Mengdi and Zhao, Xufeng and Lee, Jae Hee and Weber, Cornelius and Wermter, Stefan},  
year={2023},
journal={arXiv preprint arXiv:2302.00270}
}

                    </textarea>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
